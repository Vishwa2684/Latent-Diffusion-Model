{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/conditional-DDPM\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: diffusers in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.31.0)\n",
      "Requirement already satisfied: transformers in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.47.0)\n",
      "Requirement already satisfied: tqdm in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: importlib-metadata in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from diffusers) (8.5.0)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from diffusers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from diffusers) (0.26.5)\n",
      "Requirement already satisfied: numpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from diffusers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from diffusers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from diffusers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from diffusers) (0.4.5)\n",
      "Requirement already satisfied: Pillow in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from diffusers) (11.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (4.12.2)\n",
      "Requirement already satisfied: zipp>=3.20 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from importlib-metadata->diffusers) (3.21.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->diffusers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->diffusers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->diffusers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->diffusers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install diffusers transformers tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision.transforms import transforms\n",
    "from diffusers import UNet2DConditionModel, DDPMScheduler, AutoencoderKL\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parameters\n",
    "batch_size = 8  # Reduced batch size for stability\n",
    "epochs = 2\n",
    "learning_rate = 1e-4\n",
    "image_size = 64  # Increased image size\n",
    "latent_size = image_size // 8  # Calculate latent size (512 -> 64)\n",
    "data_path = \"/teamspace/studios/this_studio/coco\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenizer and Transforms\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "max_length = 77\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CocoWithAnnotations(Dataset):\n",
    "    def __init__(self, path, tokenizer, transform, train=True):\n",
    "        super().__init__()\n",
    "        self.path = path\n",
    "        self.data = None\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.train = train\n",
    "        if self.data is None:\n",
    "            self.open_json()\n",
    "\n",
    "    def open_json(self):\n",
    "        split = \"train\" if self.train else \"val\"\n",
    "        print(f'Loading {split} annotations...')\n",
    "        with open(f'{self.path}/annotations/captions_{split}2014.json', 'r') as stream:\n",
    "            self.data = json.load(stream)['annotations']\n",
    "        print('Annotations loaded')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        annot = self.data[index]\n",
    "        img_id = str(annot['image_id']).zfill(12)\n",
    "        split = \"train\" if self.train else \"val\"\n",
    "        \n",
    "        try:\n",
    "            image_path = f'{self.path}/{split}2014/COCO_{split}2014_{img_id}.jpg'\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image = self.transform(image)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {e}\")\n",
    "            # Return a default item in case of error\n",
    "            return torch.zeros((3, image_size, image_size)), torch.zeros((max_length,))\n",
    "\n",
    "        text_emb = self.tokenizer(\n",
    "            annot['caption'],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return image, text_emb.input_ids.squeeze(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train annotations...\n",
      "Annotations loaded\n"
     ]
    }
   ],
   "source": [
    "# Models with proper configuration\n",
    "unet = UNet2DConditionModel(\n",
    "    sample_size=latent_size,  # Use calculated latent size\n",
    "    in_channels=4,\n",
    "    out_channels=4,\n",
    "    layers_per_block=2,\n",
    "    block_out_channels=(128, 256, 512, 512),\n",
    "    down_block_types=(\n",
    "        \"CrossAttnDownBlock2D\",\n",
    "        \"CrossAttnDownBlock2D\",\n",
    "        \"CrossAttnDownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",\n",
    "        \"CrossAttnUpBlock2D\",\n",
    "        \"CrossAttnUpBlock2D\",\n",
    "        \"CrossAttnUpBlock2D\",\n",
    "    ),\n",
    "    cross_attention_dim=512,  # Match CLIP's hidden dimension\n",
    ").to(device)\n",
    "\n",
    "# Load pretrained models\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "autoencoder = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\").to(device)\n",
    "diffusion = DDPMScheduler(num_train_timesteps=1000, beta_schedule=\"linear\")\n",
    "\n",
    "# Freeze VAE and text encoder\n",
    "for param in autoencoder.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in text_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Dataset and DataLoader with error handling\n",
    "def collate_fn(batch):\n",
    "    # Filter out any None values from failed loads\n",
    "    batch = [(img, cap) for img, cap in batch if img is not None and cap is not None]\n",
    "    if not batch:\n",
    "        return None\n",
    "    images, captions = zip(*batch)\n",
    "    return torch.stack(images), torch.stack(captions)\n",
    "\n",
    "dataset = CocoWithAnnotations(data_path, tokenizer, transform, train=True)\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3ff366828b34bbfa7a9f209496da460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/2:   0%|          | 0/51765 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed with average loss: 0.1953\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3fd26f56453428881ed97f26e34af35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/2:   0%|          | 0/51765 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 completed with average loss: 0.1902\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "# Optimizer with gradient clipping\n",
    "optimizer = torch.optim.AdamW(unet.parameters(), lr=learning_rate)\n",
    "scaler = torch.cuda.amp.GradScaler()  # For mixed precision training\n",
    "\n",
    "# Training loop with improved error handling and mixed precision\n",
    "for epoch in range(epochs):\n",
    "    unet.train()\n",
    "    epoch_loss = 0\n",
    "    valid_batches = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    for batch in progress_bar:\n",
    "        if batch is None:\n",
    "            continue\n",
    "            \n",
    "        images, captions = batch\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # Encode images\n",
    "            with torch.no_grad():\n",
    "                latents = autoencoder.encode(images).latent_dist.sample()\n",
    "                latents = latents * 0.18215\n",
    "\n",
    "            # Noise augmentation\n",
    "            noise = torch.randn_like(latents)\n",
    "            timesteps = torch.randint(0, diffusion.config.num_train_timesteps, (latents.shape[0],), device=device)\n",
    "            noisy_latents = diffusion.add_noise(latents, noise, timesteps)\n",
    "\n",
    "            # Get text embeddings\n",
    "            with torch.no_grad():\n",
    "                encoder_hidden_states = text_encoder(captions)[0]\n",
    "\n",
    "            # Predict noise\n",
    "            noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "            loss = torch.nn.functional.mse_loss(noise_pred, noise, reduction=\"mean\")\n",
    "\n",
    "        # Backward pass with gradient scaling\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(unet.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        valid_batches += 1\n",
    "        progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    avg_loss = epoch_loss / valid_batches\n",
    "    print(f\"Epoch {epoch + 1} completed with average loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Save checkpoint after each epoch\n",
    "    checkpoint_dir = f\"./ldm_checkpoints/epoch_{epoch + 1}\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    unet.save_pretrained(os.path.join(checkpoint_dir, \"unet\"))\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': unet.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': avg_loss,\n",
    "    }, os.path.join(checkpoint_dir, \"training_state.pth\"))\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
