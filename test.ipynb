{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vishw\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision.transforms import transforms\n",
    "from diffusers import UNet2DConditionModel, DDPMScheduler, AutoencoderKL\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parameters\n",
    "batch_size = 8  # Reduced batch size for stability\n",
    "epochs = 2\n",
    "learning_rate = 1e-4\n",
    "image_size = 256  # Increased image size\n",
    "latent_size = image_size // 8  # Calculate latent size (512 -> 64)\n",
    "data_path = \"./coco\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenizer and Transforms\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "max_length = 77\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CocoWithAnnotations(Dataset):\n",
    "    def __init__(self, path, tokenizer, transform, train=True):\n",
    "        super().__init__()\n",
    "        self.path = path\n",
    "        self.data = None\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.train = train\n",
    "        if self.data is None:\n",
    "            self.open_json()\n",
    "\n",
    "    def open_json(self):\n",
    "        split = \"train\" if self.train else \"val\"\n",
    "        print(f'Loading {split} annotations...')\n",
    "        with open(f'{self.path}/annotations/captions_{split}2014.json', 'r') as stream:\n",
    "            self.data = json.load(stream)['annotations']\n",
    "        print('Annotations loaded')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        annot = self.data[index]\n",
    "        img_id = str(annot['image_id']).zfill(12)\n",
    "        split = \"train\" if self.train else \"val\"\n",
    "        \n",
    "        try:\n",
    "            image_path = f'{self.path}/{split}2014/COCO_{split}2014_{img_id}.jpg'\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image = self.transform(image)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {e}\")\n",
    "            # Return a default item in case of error\n",
    "            return torch.zeros((3, image_size, image_size)), torch.zeros((max_length,))\n",
    "\n",
    "        text_emb = self.tokenizer(\n",
    "            annot['caption'],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return image, text_emb.input_ids.squeeze(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train annotations...\n",
      "Annotations loaded\n"
     ]
    }
   ],
   "source": [
    "# Models with proper configuration\n",
    "unet = UNet2DConditionModel(\n",
    "    sample_size=latent_size,  # Use calculated latent size\n",
    "    in_channels=4,\n",
    "    out_channels=4,\n",
    "    layers_per_block=2,\n",
    "    block_out_channels=(128, 256, 512, 512),\n",
    "    down_block_types=(\n",
    "        \"CrossAttnDownBlock2D\",\n",
    "        \"CrossAttnDownBlock2D\",\n",
    "        \"CrossAttnDownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",\n",
    "        \"CrossAttnUpBlock2D\",\n",
    "        \"CrossAttnUpBlock2D\",\n",
    "        \"CrossAttnUpBlock2D\",\n",
    "    ),\n",
    "    cross_attention_dim=768,  # Match CLIP's hidden dimension\n",
    ").to(device)\n",
    "\n",
    "# Load pretrained models\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "autoencoder = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\").to(device)\n",
    "diffusion = DDPMScheduler(num_train_timesteps=1000, beta_schedule=\"linear\")\n",
    "\n",
    "# Freeze VAE and text encoder\n",
    "for param in autoencoder.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in text_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Dataset and DataLoader with error handling\n",
    "def collate_fn(batch):\n",
    "    # Filter out any None values from failed loads\n",
    "    batch = [(img, cap) for img, cap in batch if img is not None and cap is not None]\n",
    "    if not batch:\n",
    "        return None\n",
    "    images, captions = zip(*batch)\n",
    "    return torch.stack(images), torch.stack(captions)\n",
    "\n",
    "dataset = CocoWithAnnotations(data_path, tokenizer, transform, train=True)\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vishw\\AppData\\Local\\Temp\\ipykernel_13376\\1185090775.py:3: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()  # For mixed precision training\n",
      "Epoch 1/2:   0%|          | 0/51765 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Optimizer with gradient clipping\n",
    "optimizer = torch.optim.AdamW(unet.parameters(), lr=learning_rate)\n",
    "scaler = torch.cuda.amp.GradScaler()  # For mixed precision training\n",
    "\n",
    "# Training loop with improved error handling and mixed precision\n",
    "for epoch in range(epochs):\n",
    "    unet.train()\n",
    "    epoch_loss = 0\n",
    "    valid_batches = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    for batch in progress_bar:\n",
    "        if batch is None:\n",
    "            continue\n",
    "            \n",
    "        images, captions = batch\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # Encode images\n",
    "            with torch.no_grad():\n",
    "                latents = autoencoder.encode(images).latent_dist.sample()\n",
    "                latents = latents * 0.18215\n",
    "\n",
    "            # Noise augmentation\n",
    "            noise = torch.randn_like(latents)\n",
    "            timesteps = torch.randint(0, diffusion.config.num_train_timesteps, (latents.shape[0],), device=device)\n",
    "            noisy_latents = diffusion.add_noise(latents, noise, timesteps)\n",
    "\n",
    "            # Get text embeddings\n",
    "            with torch.no_grad():\n",
    "                encoder_hidden_states = text_encoder(captions)[0]\n",
    "\n",
    "            # Predict noise\n",
    "            noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "            loss = torch.nn.functional.mse_loss(noise_pred, noise, reduction=\"mean\")\n",
    "\n",
    "        # Backward pass with gradient scaling\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(unet.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        valid_batches += 1\n",
    "        progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    avg_loss = epoch_loss / valid_batches\n",
    "    print(f\"Epoch {epoch + 1} completed with average loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Save checkpoint after each epoch\n",
    "    checkpoint_dir = f\"./ldm_checkpoints/epoch_{epoch + 1}\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    unet.save_pretrained(os.path.join(checkpoint_dir, \"unet\"))\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': unet.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': avg_loss,\n",
    "    }, os.path.join(checkpoint_dir, \"training_state.pth\"))\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
