{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vishw\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision.transforms import transforms\n",
    "from diffusers import UNet2DConditionModel, DDPMScheduler,AutoencoderKL\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parameters\n",
    "batch_size = 16\n",
    "epochs = 2\n",
    "learning_rate = 1e-4\n",
    "image_size = 32\n",
    "data_path = \"./coco\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenizer and Transforms\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "max_length =77\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================= Loading training annotations =======================\n",
      "======================= ANNOTATIONS LOADED =======================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Dataset Class\n",
    "class CocoWithAnnotations(Dataset):\n",
    "    def __init__(self, path, tokenizer, transform, train=True):\n",
    "        super().__init__()\n",
    "        self.path = path\n",
    "        self.data = None\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.train = train\n",
    "        if self.data is None:\n",
    "            self.open_json()\n",
    "\n",
    "    def open_json(self):\n",
    "        if self.train:\n",
    "            print('======================= Loading training annotations =======================')\n",
    "            with open(f'{self.path}/annotations/captions_train2014.json', 'r') as stream:\n",
    "                self.data = json.load(stream)\n",
    "            self.data = self.data['annotations']\n",
    "        else:\n",
    "            print('======================= Loading validation annotations =======================')\n",
    "            with open(f'{self.path}/annotations/captions_val2014.json', 'r') as stream:\n",
    "                self.data = json.load(stream)\n",
    "            self.data = self.data['annotations']\n",
    "        print('======================= ANNOTATIONS LOADED =======================')\n",
    "    # Dataset __getitem__ method\n",
    "    def __getitem__(self, index):\n",
    "        annot = self.data[index]\n",
    "        if len(str(annot['image_id'])) < 6:\n",
    "            rem_0l = 6 - len(str(annot['image_id']))\n",
    "            rem_0 = '0' * rem_0l\n",
    "            image = self.transform(\n",
    "                Image.open(f'{self.path}/train2014/COCO_train2014_000000{rem_0 + str(annot[\"image_id\"])}.jpg').convert('RGB')\n",
    "            )\n",
    "        else:\n",
    "            image = self.transform(\n",
    "                Image.open(f'{self.path}/train2014/COCO_train2014_000000{annot[\"image_id\"]}.jpg').convert('RGB')\n",
    "            )\n",
    "\n",
    "        text_emb = self.tokenizer(\n",
    "            annot['caption'],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_length,  # Ensure correct sequence length\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return image, text_emb.input_ids.squeeze(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = CocoWithAnnotations(data_path, tokenizer, transform, train=True)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Models\n",
    "autoencoder = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\").to(device)\n",
    "unet = UNet2DConditionModel(\n",
    "    sample_size=image_size,\n",
    "    in_channels=4,\n",
    "    out_channels=4,\n",
    "    down_block_types=(\n",
    "        'DownBlock2D',\n",
    "        'CrossAttnDownBlock2D',\n",
    "        'CrossAttnDownBlock2D'\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        'CrossAttnUpBlock2D',\n",
    "        'CrossAttnUpBlock2D',\n",
    "        'UpBlock2D'\n",
    "    ),\n",
    "    block_out_channels=(64, 128, 256),\n",
    "    cross_attention_dim=512  # Ensure this matches CLIP hidden_dim\n",
    ").to(device)\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Scheduler\n",
    "diffusion = DDPMScheduler(num_train_timesteps=1000, beta_schedule=\"linear\")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(unet.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:   0%|          | 0/25883 [00:00<?, ?it/s]C:\\Users\\vishw\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\diffusers\\configuration_utils.py:140: FutureWarning: Accessing config attribute `num_train_timesteps` directly via 'DDPMScheduler' object attribute is deprecated. Please access 'num_train_timesteps' over 'DDPMScheduler's config object instead, e.g. 'scheduler.config.num_train_timesteps'.\n",
      "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
      "Epoch 1/2: 100%|██████████| 25883/25883 [1:28:19<00:00,  4.88it/s, loss=0.00679]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed with loss: 0.0068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|██████████| 25883/25883 [1:33:57<00:00,  4.59it/s, loss=0.00188]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 completed with loss: 0.0019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    unet.train()\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    for images, captions in progress_bar:\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        # Encode images to latent space\n",
    "        latents = autoencoder.encode(images).latent_dist.sample() * 0.18215\n",
    "\n",
    "        # Forward diffusion\n",
    "        noise = torch.randn_like(latents).to(device)\n",
    "        timesteps = torch.randint(0, diffusion.num_train_timesteps, (latents.size(0),), device=device).long()\n",
    "        noisy_latents = diffusion.add_noise(latents, noise, timesteps)\n",
    "\n",
    "        # Encode text\n",
    "        text_embeds = text_encoder(captions).last_hidden_state\n",
    "\n",
    "        # Predict noise\n",
    "        noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states=text_embeds).sample\n",
    "\n",
    "        # Loss\n",
    "        loss = torch.nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} completed with loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "output_dir = \"./latent_diffusion_model\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "unet.save_pretrained(output_dir)\n",
    "text_encoder.save_pretrained(output_dir)\n",
    "autoencoder.save_pretrained(output_dir)\n",
    "\n",
    "diffusion.save_config(os.path.join(output_dir, \"scheduler_config.json\"))\n",
    "\n",
    "print(\"Model saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
