{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vishw\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "from transformers import CLIPTokenizer,CLIPTextModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import transforms\n",
    "from PIL import Image\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                                transforms.Resize((256,256)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5]),\n",
    "                                ])\n",
    "\n",
    "class CocoWithAnnotations(Dataset):\n",
    "    def __init__(self,path,tokenizer,text_model,transform,train=True):\n",
    "        super().__init__()\n",
    "        self.path = path\n",
    "        self.data = None\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_model = text_model\n",
    "        self.train = train\n",
    "        if self.data is None:\n",
    "            self.open_json()\n",
    "    \n",
    "    def open_json(self):\n",
    "        if self.train:\n",
    "            print('======================= Loading training annotations =======================')\n",
    "            with open(f'{self.path}/annotations/captions_train2014.json','r+') as stream:\n",
    "                self.data = json.load(stream)\n",
    "            self.data = self.data['annotations']\n",
    "        else:\n",
    "            print('======================= Loading validation annotations =======================')\n",
    "            with open(f'./{self.path}/annotations/captions_val2014.json','r+') as stream:\n",
    "                self.data = json.load(stream)\n",
    "            self.data = self.data['annotations']\n",
    "        print('======================= ANNOTATIONS LOADED =======================')\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        annot = self.data[index]\n",
    "        image_id =str(annot[\"image_id\"]).zfill(6)\n",
    "        image = self.transform(Image.open(f'{self.path}/train2014/COCO_train2014_000000{image_id}.jpg'))\n",
    "        tokens = self.tokenizer(annot['caption'], padding=\"max_length\", truncation=True, max_length=77, return_tensors=\"pt\")\n",
    "        text_embs = self.text_model(**tokens).last_hidden_state\n",
    "        return image,text_embs.squeeze(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_beta_schedule(timesteps):\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps)\n",
    "\n",
    "# Define alpha values (cumulative product of 1 - beta)\n",
    "def compute_alpha_cumprod(beta_schedule):\n",
    "    alpha = 1.0 - beta_schedule\n",
    "    alpha_cumprod = torch.cumprod(alpha, dim=0)\n",
    "    return alpha_cumprod\n",
    "\n",
    "# Define timesteps and compute schedule\n",
    "timesteps = 1000\n",
    "beta_schedule = linear_beta_schedule(timesteps)\n",
    "alpha_cumprod = compute_alpha_cumprod(beta_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================= Loading training annotations =======================\n",
      "======================= ANNOTATIONS LOADED =======================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained('openai/clip-vit-base-patch32')\n",
    "clip_text_model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "dataset = CocoWithAnnotations('./coco',tokenizer,clip_text_model,transform)\n",
    "\n",
    "# Example text\n",
    "captions = \"a black and white cat in a basket on a sofa\"\n",
    "\n",
    "# Tokenize text (output has input IDs and attention masks)\n",
    "tokenized_captions = tokenizer(captions, padding=\"max_length\", truncation=True, max_length=77, return_tensors=\"pt\")\n",
    "\n",
    "# Extract text embeddings\n",
    "text_embeddings = clip_text_model(**tokenized_captions).last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.6078, -0.6157, -0.6157,  ..., -0.6627, -0.7569, -0.6863],\n",
       "          [-0.6000, -0.6157, -0.6078,  ..., -0.3647, -0.5922, -0.6784],\n",
       "          [-0.5843, -0.6078, -0.6157,  ..., -0.4745, -0.5608, -0.6706],\n",
       "          ...,\n",
       "          [-0.0039, -0.0039,  0.0196,  ..., -0.1451, -0.1608, -0.1765],\n",
       "          [-0.0431, -0.0431, -0.0275,  ..., -0.1686, -0.1843, -0.1922],\n",
       "          [-0.0431, -0.0431, -0.0353,  ..., -0.1922, -0.2078, -0.2235]],\n",
       " \n",
       "         [[-0.3569, -0.3569, -0.3490,  ..., -0.6157, -0.6863, -0.5843],\n",
       "          [-0.3333, -0.3255, -0.3176,  ..., -0.3647, -0.5686, -0.5608],\n",
       "          [-0.3176, -0.2941, -0.2863,  ..., -0.4510, -0.5451, -0.5765],\n",
       "          ...,\n",
       "          [-0.0275, -0.0353, -0.0275,  ..., -0.1843, -0.2000, -0.2000],\n",
       "          [-0.0667, -0.0667, -0.0588,  ..., -0.2157, -0.2235, -0.2314],\n",
       "          [-0.0667, -0.0667, -0.0588,  ..., -0.2314, -0.2471, -0.2471]],\n",
       " \n",
       "         [[-0.1529, -0.1373, -0.1294,  ..., -0.5765, -0.5922, -0.5373],\n",
       "          [-0.0980, -0.1137, -0.1059,  ..., -0.3176, -0.5059, -0.5216],\n",
       "          [-0.0824, -0.0902, -0.0902,  ..., -0.4039, -0.4745, -0.4980],\n",
       "          ...,\n",
       "          [-0.0824, -0.0824, -0.0745,  ..., -0.1922, -0.2000, -0.2157],\n",
       "          [-0.1216, -0.1216, -0.1137,  ..., -0.2157, -0.2392, -0.2471],\n",
       "          [-0.1216, -0.1216, -0.1137,  ..., -0.2392, -0.2471, -0.2549]]]),\n",
       " tensor([[ 0.3393,  0.1165,  0.1020,  ...,  0.2468,  0.5906,  0.1013],\n",
       "         [ 1.9753, -0.5844,  0.3685,  ...,  1.1658,  0.8050, -0.9801],\n",
       "         [-0.5007, -0.5874, -0.3478,  ..., -0.0100, -0.0028, -1.6169],\n",
       "         ...,\n",
       "         [ 0.9729, -0.0171, -0.5920,  ...,  0.2325,  0.9527,  1.3100],\n",
       "         [ 0.9867,  0.0385, -0.5661,  ...,  0.2271,  0.9210,  1.2840],\n",
       "         [ 0.8884,  0.0763, -0.3658,  ...,  0.1145,  0.9910,  1.3297]],\n",
       "        grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(dataset_cell):\n",
    "    import matplotlib.pyplot as plt\n",
    "    inv_transform = transforms.Compose([\n",
    "        transforms.Normalize(mean=[-0.5 / 0.5, -0.5 / 0.5, -0.5 / 0.5], std=[1 / 0.5, 1 / 0.5, 1 / 0.5]),\n",
    "        transforms.ToPILImage(),\n",
    "    ])\n",
    "\n",
    "    image = inv_transform(dataset_cell[0])\n",
    "    tokens,attentin_masks = dataset_cell[1]['input_ids'],dataset_cell[1]['attention_mask']\n",
    "    print(attentin_masks)\n",
    "    # text =tokenizer.decode(tokens,skip_special_tokens=True)\n",
    "    # plt.title(text)\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
