{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import UNet2DConditionModel, DDPMScheduler, AutoencoderKL\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapeDebugger:\n",
    "    @staticmethod\n",
    "    def check_shape(tensor, expected_shape, layer_name):\n",
    "        if tensor.shape != expected_shape:\n",
    "            raise ValueError(\n",
    "                f\"Shape mismatch in {layer_name}:\\n\"\n",
    "                f\"Expected shape: {expected_shape}\\n\"\n",
    "                f\"Got shape: {tensor.shape}\"\n",
    "            )\n",
    "        print(f\"✓ {layer_name} shape: {tensor.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(model_path):\n",
    "    \"\"\"Load all required models from the saved path\"\"\"\n",
    "    print(\"Loading models...\")\n",
    "    \n",
    "    # Load models from saved weights\n",
    "    unet = UNet2DConditionModel.from_pretrained(model_path, low_cpu_mem_usage=False).to(device)\n",
    "    text_encoder = CLIPTextModel.from_pretrained(model_path).to(device)\n",
    "    \n",
    "    # Load autoencoder from original Stable Diffusion weights instead of local path\n",
    "    autoencoder = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\").to(device)\n",
    "    \n",
    "    # Load tokenizer (this is usually loaded from original CLIP)\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    \n",
    "    # Load scheduler configuration\n",
    "    scheduler = DDPMScheduler.from_config(os.path.join(model_path, \"scheduler_config.json\"))\n",
    "    \n",
    "    return unet, text_encoder, autoencoder, tokenizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def generate_image(prompt, num_inference_steps=50, image_size=512, guidance_scale=7.5):\n",
    "    \"\"\"Generate an image from a text prompt with shape debugging\"\"\"\n",
    "    \n",
    "    debug = ShapeDebugger()\n",
    "    model_path = \"./latent_diffusion_model\"\n",
    "    unet, text_encoder, autoencoder, tokenizer, scheduler = load_models(model_path)\n",
    "    \n",
    "    # Set evaluation mode\n",
    "    unet.eval()\n",
    "    text_encoder.eval()\n",
    "    autoencoder.eval()\n",
    "    \n",
    "    # Text embedding pipeline with shape checking\n",
    "    text_input = tokenizer(\n",
    "        prompt,\n",
    "        padding=\"max_length\",\n",
    "        max_length=77,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    debug.check_shape(\n",
    "        text_input.input_ids, \n",
    "        torch.Size([1, 77]), \n",
    "        \"Tokenizer output\"\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        text_embeddings = text_encoder(text_input.input_ids)[0]\n",
    "        debug.check_shape(\n",
    "            text_embeddings,\n",
    "            torch.Size([1, 77, text_encoder.config.hidden_size]),\n",
    "            \"Text encoder output\"\n",
    "        )\n",
    "    \n",
    "    # Latent initialization with shape checking\n",
    "    # Changed the latent shape to match VAE expectations\n",
    "    latents = torch.randn(\n",
    "        (1, 4, image_size // 8, image_size // 8),  # Changed to maintain aspect ratio\n",
    "        device=device\n",
    "    )\n",
    "    debug.check_shape(\n",
    "        latents,\n",
    "        torch.Size([1, 4, image_size // 8, image_size // 8]),\n",
    "        \"Initial latents\"\n",
    "    )\n",
    "    \n",
    "    # Denoising loop with shape checking\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "    latents = latents * scheduler.init_noise_sigma\n",
    "    \n",
    "    for t in scheduler.timesteps:\n",
    "        print(f\"\\nDenoising step {t}\")\n",
    "        \n",
    "        latent_model_input = scheduler.scale_model_input(latents, timestep=t)\n",
    "        debug.check_shape(\n",
    "            latent_model_input,\n",
    "            torch.Size([1, 4, image_size // 8, image_size // 8]),\n",
    "            \"Scaled model input\"\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                noise_pred = unet(\n",
    "                    latent_model_input,\n",
    "                    t,\n",
    "                    encoder_hidden_states=text_embeddings\n",
    "                ).sample\n",
    "                \n",
    "                debug.check_shape(\n",
    "                    noise_pred,\n",
    "                    torch.Size([1, 4, image_size // 8, image_size // 8]),\n",
    "                    \"UNet output\"\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(\"\\nError in UNet forward pass:\")\n",
    "                print(f\"Input shapes:\")\n",
    "                print(f\"- latent_model_input: {latent_model_input.shape}\")\n",
    "                print(f\"- timestep: {t.shape if isinstance(t, torch.Tensor) else 'scalar'}\")\n",
    "                print(f\"- text_embeddings: {text_embeddings.shape}\")\n",
    "                raise e\n",
    "        \n",
    "        latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "        debug.check_shape(\n",
    "            latents,\n",
    "            torch.Size([1, 4, image_size // 8, image_size // 8]),\n",
    "            \"Updated latents\"\n",
    "        )\n",
    "    \n",
    "    # Decoding with shape checking\n",
    "    with torch.no_grad():\n",
    "        latents = 1 / 0.18215 * latents\n",
    "        debug.check_shape(\n",
    "            latents,\n",
    "            torch.Size([1, 4, image_size // 8, image_size // 8]),\n",
    "            \"Scaled latents for decoder\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            image = autoencoder.decode(latents).sample\n",
    "            debug.check_shape(\n",
    "                image,\n",
    "                torch.Size([1, 3, image_size, image_size]),\n",
    "                \"Decoded image\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\"\\nError in VAE decoding:\")\n",
    "            print(f\"Input latents shape: {latents.shape}\")\n",
    "            raise e\n",
    "    \n",
    "    # Normalize image\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Generating image for prompt: 'a beautiful sunset over mountains'\n",
      "Loading models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./latent_diffusion_model were not used when initializing UNet2DConditionModel: ['encoder.mid_block.resnets.1.norm2.weight', 'encoder.conv_norm_out.weight', 'encoder.mid_block.resnets.0.norm1.bias', 'encoder.down_blocks.0.resnets.1.conv1.bias', 'decoder.mid_block.resnets.0.norm2.weight', 'encoder.down_blocks.3.resnets.0.conv1.weight', 'decoder.mid_block.attentions.0.to_out.0.bias', 'decoder.conv_norm_out.weight', 'decoder.up_blocks.3.resnets.2.norm2.weight', 'encoder.down_blocks.1.resnets.0.conv2.weight', 'encoder.down_blocks.3.resnets.0.conv2.bias', 'encoder.mid_block.attentions.0.to_k.bias', 'encoder.mid_block.resnets.1.conv2.weight', 'decoder.mid_block.attentions.0.group_norm.bias', 'encoder.down_blocks.1.resnets.0.conv_shortcut.bias', 'encoder.down_blocks.1.resnets.0.norm2.bias', 'encoder.mid_block.resnets.1.norm2.bias', 'decoder.up_blocks.1.upsamplers.0.conv.weight', 'decoder.up_blocks.2.resnets.0.conv_shortcut.weight', 'encoder.down_blocks.3.resnets.0.norm1.weight', 'decoder.up_blocks.0.upsamplers.0.conv.bias', 'encoder.down_blocks.3.resnets.1.conv2.bias', 'decoder.mid_block.attentions.0.to_out.0.weight', 'decoder.up_blocks.2.resnets.0.norm2.weight', 'decoder.up_blocks.2.upsamplers.0.conv.bias', 'decoder.up_blocks.1.resnets.2.conv1.bias', 'encoder.down_blocks.0.resnets.0.norm1.bias', 'decoder.up_blocks.0.resnets.2.norm1.bias', 'encoder.mid_block.resnets.0.conv1.weight', 'decoder.mid_block.resnets.0.norm1.bias', 'decoder.up_blocks.1.resnets.0.norm2.bias', 'decoder.mid_block.resnets.1.conv1.bias', 'encoder.conv_in.bias', 'decoder.mid_block.resnets.1.norm1.bias', 'decoder.up_blocks.3.resnets.1.norm2.bias', 'encoder.conv_out.weight', 'decoder.mid_block.attentions.0.group_norm.weight', 'decoder.up_blocks.3.resnets.1.norm1.weight', 'encoder.down_blocks.1.resnets.0.conv1.weight', 'encoder.down_blocks.3.resnets.1.norm2.weight', 'encoder.mid_block.attentions.0.to_out.0.bias', 'encoder.mid_block.resnets.1.conv2.bias', 'decoder.up_blocks.2.resnets.2.norm1.weight', 'decoder.up_blocks.0.resnets.1.norm2.bias', 'decoder.up_blocks.2.resnets.2.norm2.bias', 'decoder.up_blocks.3.resnets.0.conv2.weight', 'decoder.up_blocks.2.resnets.2.norm1.bias', 'encoder.mid_block.attentions.0.to_q.bias', 'decoder.up_blocks.0.resnets.2.conv1.bias', 'encoder.mid_block.resnets.0.conv1.bias', 'decoder.up_blocks.3.resnets.0.conv2.bias', 'decoder.up_blocks.1.resnets.0.conv2.weight', 'decoder.up_blocks.1.resnets.0.conv1.weight', 'decoder.conv_in.bias', 'decoder.up_blocks.3.resnets.1.norm1.bias', 'decoder.up_blocks.0.resnets.2.norm1.weight', 'encoder.down_blocks.0.resnets.1.conv2.weight', 'encoder.mid_block.resnets.0.conv2.bias', 'decoder.up_blocks.0.resnets.2.conv2.bias', 'encoder.down_blocks.0.resnets.1.norm2.weight', 'encoder.mid_block.resnets.1.norm1.bias', 'encoder.mid_block.resnets.1.norm1.weight', 'decoder.up_blocks.0.resnets.1.norm1.weight', 'encoder.mid_block.resnets.0.norm2.weight', 'decoder.conv_out.weight', 'quant_conv.bias', 'encoder.mid_block.attentions.0.to_out.0.weight', 'encoder.down_blocks.0.resnets.1.conv1.weight', 'decoder.up_blocks.0.resnets.1.conv2.bias', 'encoder.down_blocks.3.resnets.1.norm1.weight', 'encoder.mid_block.resnets.1.conv1.weight', 'decoder.up_blocks.3.resnets.2.conv2.bias', 'decoder.up_blocks.1.resnets.2.conv1.weight', 'decoder.up_blocks.3.resnets.0.norm1.weight', 'encoder.mid_block.resnets.0.norm1.weight', 'encoder.mid_block.attentions.0.to_v.bias', 'decoder.up_blocks.1.resnets.1.norm1.weight', 'decoder.mid_block.attentions.0.to_v.bias', 'decoder.mid_block.attentions.0.to_k.weight', 'encoder.down_blocks.1.downsamplers.0.conv.weight', 'encoder.down_blocks.1.resnets.0.conv_shortcut.weight', 'encoder.down_blocks.0.downsamplers.0.conv.weight', 'encoder.down_blocks.1.resnets.0.conv1.bias', 'decoder.up_blocks.3.resnets.2.norm1.weight', 'decoder.up_blocks.1.resnets.1.conv1.weight', 'decoder.up_blocks.2.resnets.1.norm1.bias', 'decoder.up_blocks.3.resnets.2.conv2.weight', 'encoder.down_blocks.1.resnets.1.conv1.bias', 'encoder.down_blocks.2.resnets.1.conv1.weight', 'encoder.down_blocks.3.resnets.1.conv1.bias', 'decoder.up_blocks.1.resnets.1.conv1.bias', 'decoder.up_blocks.2.resnets.2.norm2.weight', 'encoder.down_blocks.2.resnets.0.conv1.weight', 'encoder.down_blocks.2.resnets.0.conv2.weight', 'decoder.up_blocks.3.resnets.0.norm2.bias', 'decoder.up_blocks.1.resnets.2.norm1.bias', 'decoder.up_blocks.3.resnets.0.conv1.bias', 'decoder.mid_block.resnets.0.norm1.weight', 'encoder.down_blocks.3.resnets.1.norm2.bias', 'decoder.up_blocks.1.resnets.0.norm1.bias', 'decoder.conv_out.bias', 'encoder.down_blocks.2.resnets.0.conv_shortcut.bias', 'decoder.mid_block.resnets.1.norm2.bias', 'encoder.down_blocks.1.resnets.0.norm2.weight', 'decoder.up_blocks.2.resnets.2.conv2.weight', 'decoder.mid_block.resnets.0.conv2.bias', 'decoder.up_blocks.2.resnets.0.conv2.weight', 'decoder.up_blocks.0.resnets.1.norm1.bias', 'decoder.up_blocks.2.resnets.0.conv_shortcut.bias', 'decoder.mid_block.resnets.1.conv1.weight', 'decoder.up_blocks.2.resnets.0.norm1.weight', 'encoder.conv_norm_out.bias', 'encoder.down_blocks.0.resnets.0.conv2.bias', 'decoder.up_blocks.0.resnets.0.conv1.weight', 'decoder.up_blocks.3.resnets.2.norm1.bias', 'encoder.down_blocks.1.resnets.0.conv2.bias', 'encoder.mid_block.attentions.0.to_v.weight', 'decoder.up_blocks.1.resnets.2.norm1.weight', 'decoder.up_blocks.0.resnets.0.norm1.weight', 'decoder.up_blocks.1.resnets.2.norm2.weight', 'encoder.down_blocks.2.resnets.0.conv2.bias', 'decoder.up_blocks.0.resnets.2.norm2.weight', 'encoder.down_blocks.2.downsamplers.0.conv.bias', 'decoder.mid_block.resnets.1.norm2.weight', 'decoder.up_blocks.2.resnets.1.norm1.weight', 'encoder.down_blocks.2.resnets.0.norm1.weight', 'decoder.conv_norm_out.bias', 'decoder.up_blocks.3.resnets.0.norm2.weight', 'encoder.down_blocks.0.resnets.0.norm2.bias', 'decoder.up_blocks.0.resnets.0.norm1.bias', 'encoder.down_blocks.2.resnets.0.conv_shortcut.weight', 'encoder.down_blocks.2.resnets.1.norm1.bias', 'decoder.up_blocks.2.resnets.0.conv1.weight', 'encoder.down_blocks.3.resnets.0.conv2.weight', 'decoder.mid_block.resnets.1.conv2.weight', 'decoder.up_blocks.0.resnets.0.conv2.weight', 'decoder.mid_block.attentions.0.to_k.bias', 'decoder.up_blocks.1.resnets.2.norm2.bias', 'decoder.up_blocks.2.upsamplers.0.conv.weight', 'decoder.up_blocks.3.resnets.1.norm2.weight', 'encoder.down_blocks.2.resnets.1.norm2.bias', 'encoder.down_blocks.2.resnets.1.conv2.weight', 'encoder.down_blocks.2.resnets.1.norm2.weight', 'decoder.up_blocks.1.resnets.2.conv2.bias', 'decoder.up_blocks.0.resnets.2.conv2.weight', 'encoder.down_blocks.3.resnets.0.conv1.bias', 'encoder.down_blocks.3.resnets.1.conv2.weight', 'decoder.up_blocks.1.resnets.0.conv2.bias', 'encoder.conv_out.bias', 'encoder.mid_block.resnets.1.conv1.bias', 'decoder.mid_block.resnets.1.norm1.weight', 'encoder.down_blocks.3.resnets.1.conv1.weight', 'decoder.up_blocks.0.resnets.2.conv1.weight', 'decoder.up_blocks.2.resnets.0.conv1.bias', 'decoder.up_blocks.2.resnets.0.conv2.bias', 'encoder.down_blocks.1.resnets.1.norm1.bias', 'decoder.up_blocks.1.resnets.1.conv2.bias', 'decoder.up_blocks.2.resnets.0.norm1.bias', 'encoder.down_blocks.1.resnets.1.conv2.weight', 'encoder.down_blocks.2.resnets.1.norm1.weight', 'encoder.down_blocks.0.resnets.1.norm2.bias', 'decoder.mid_block.resnets.0.norm2.bias', 'decoder.up_blocks.2.resnets.1.conv2.weight', 'decoder.up_blocks.2.resnets.2.conv2.bias', 'decoder.mid_block.resnets.1.conv2.bias', 'decoder.up_blocks.3.resnets.2.conv1.bias', 'decoder.up_blocks.1.resnets.0.conv1.bias', 'post_quant_conv.weight', 'encoder.down_blocks.3.resnets.0.norm2.weight', 'decoder.up_blocks.3.resnets.2.norm2.bias', 'decoder.up_blocks.0.resnets.0.norm2.bias', 'decoder.up_blocks.0.upsamplers.0.conv.weight', 'encoder.down_blocks.3.resnets.0.norm1.bias', 'encoder.mid_block.attentions.0.to_q.weight', 'encoder.mid_block.resnets.0.conv2.weight', 'encoder.down_blocks.1.resnets.1.conv1.weight', 'encoder.mid_block.attentions.0.group_norm.weight', 'decoder.up_blocks.2.resnets.1.norm2.weight', 'decoder.up_blocks.2.resnets.1.conv1.bias', 'encoder.down_blocks.2.resnets.1.conv2.bias', 'decoder.up_blocks.1.upsamplers.0.conv.bias', 'decoder.up_blocks.2.resnets.1.conv1.weight', 'encoder.down_blocks.1.resnets.1.norm2.weight', 'decoder.mid_block.attentions.0.to_q.weight', 'quant_conv.weight', 'decoder.mid_block.resnets.0.conv1.bias', 'encoder.down_blocks.1.resnets.0.norm1.bias', 'encoder.down_blocks.3.resnets.1.norm1.bias', 'decoder.up_blocks.3.resnets.1.conv1.weight', 'decoder.up_blocks.1.resnets.1.norm2.bias', 'encoder.down_blocks.2.resnets.0.conv1.bias', 'encoder.down_blocks.3.resnets.0.norm2.bias', 'encoder.mid_block.attentions.0.group_norm.bias', 'decoder.mid_block.attentions.0.to_v.weight', 'decoder.mid_block.resnets.0.conv2.weight', 'decoder.up_blocks.3.resnets.0.conv1.weight', 'post_quant_conv.bias', 'decoder.up_blocks.3.resnets.1.conv2.weight', 'decoder.up_blocks.0.resnets.0.conv2.bias', 'decoder.up_blocks.1.resnets.1.norm1.bias', 'decoder.up_blocks.3.resnets.0.conv_shortcut.bias', 'encoder.down_blocks.1.resnets.1.norm1.weight', 'encoder.down_blocks.0.resnets.1.norm1.weight', 'encoder.down_blocks.2.downsamplers.0.conv.weight', 'decoder.up_blocks.0.resnets.1.conv1.weight', 'encoder.down_blocks.1.resnets.1.conv2.bias', 'encoder.down_blocks.0.resnets.0.conv1.weight', 'decoder.up_blocks.2.resnets.0.norm2.bias', 'decoder.mid_block.resnets.0.conv1.weight', 'encoder.down_blocks.2.resnets.0.norm1.bias', 'decoder.up_blocks.3.resnets.0.conv_shortcut.weight', 'decoder.up_blocks.3.resnets.1.conv1.bias', 'encoder.down_blocks.0.downsamplers.0.conv.bias', 'encoder.mid_block.attentions.0.to_k.weight', 'encoder.down_blocks.0.resnets.0.norm1.weight', 'decoder.up_blocks.1.resnets.0.norm2.weight', 'decoder.up_blocks.1.resnets.1.conv2.weight', 'decoder.up_blocks.0.resnets.1.norm2.weight', 'decoder.up_blocks.1.resnets.2.conv2.weight', 'decoder.up_blocks.3.resnets.0.norm1.bias', 'encoder.down_blocks.2.resnets.0.norm2.weight', 'encoder.down_blocks.2.resnets.1.conv1.bias', 'encoder.conv_in.weight', 'decoder.up_blocks.0.resnets.0.norm2.weight', 'decoder.up_blocks.1.resnets.0.norm1.weight', 'encoder.down_blocks.1.resnets.1.norm2.bias', 'encoder.down_blocks.2.resnets.0.norm2.bias', 'decoder.up_blocks.2.resnets.2.conv1.bias', 'encoder.mid_block.resnets.0.norm2.bias', 'decoder.up_blocks.0.resnets.2.norm2.bias', 'decoder.up_blocks.0.resnets.1.conv2.weight', 'decoder.up_blocks.1.resnets.1.norm2.weight', 'decoder.up_blocks.2.resnets.1.conv2.bias', 'encoder.down_blocks.0.resnets.0.norm2.weight', 'decoder.conv_in.weight', 'decoder.up_blocks.0.resnets.1.conv1.bias', 'encoder.down_blocks.0.resnets.1.norm1.bias', 'decoder.up_blocks.0.resnets.0.conv1.bias', 'decoder.up_blocks.2.resnets.1.norm2.bias', 'decoder.up_blocks.3.resnets.1.conv2.bias', 'decoder.up_blocks.3.resnets.2.conv1.weight', 'encoder.down_blocks.1.downsamplers.0.conv.bias', 'encoder.down_blocks.0.resnets.0.conv2.weight', 'encoder.down_blocks.0.resnets.0.conv1.bias', 'decoder.up_blocks.2.resnets.2.conv1.weight', 'decoder.mid_block.attentions.0.to_q.bias', 'encoder.down_blocks.0.resnets.1.conv2.bias', 'encoder.down_blocks.1.resnets.0.norm1.weight']\n",
      "- This IS expected if you are initializing UNet2DConditionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing UNet2DConditionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of UNet2DConditionModel were not initialized from the model checkpoint at ./latent_diffusion_model and are newly initialized: ['up_blocks.1.resnets.1.norm1.bias', 'down_blocks.2.resnets.1.conv2.bias', 'up_blocks.2.resnets.2.conv2.bias', 'up_blocks.0.resnets.0.norm2.weight', 'up_blocks.2.resnets.0.norm1.bias', 'mid_block.attentions.0.transformer_blocks.0.norm2.bias', 'up_blocks.3.resnets.2.time_emb_proj.bias', 'up_blocks.2.resnets.1.conv1.bias', 'down_blocks.1.downsamplers.0.conv.bias', 'up_blocks.2.resnets.0.time_emb_proj.weight', 'down_blocks.1.resnets.0.norm1.bias', 'up_blocks.2.resnets.2.time_emb_proj.bias', 'up_blocks.2.resnets.2.norm1.weight', 'up_blocks.3.resnets.0.conv2.bias', 'up_blocks.1.upsamplers.0.conv.bias', 'down_blocks.3.resnets.0.norm2.weight', 'up_blocks.2.resnets.2.norm2.weight', 'up_blocks.1.resnets.0.time_emb_proj.weight', 'down_blocks.2.resnets.1.conv1.weight', 'down_blocks.3.resnets.0.norm1.bias', 'mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'up_blocks.1.resnets.0.norm2.weight', 'down_blocks.2.resnets.0.norm1.bias', 'up_blocks.1.resnets.2.time_emb_proj.weight', 'down_blocks.1.downsamplers.0.conv.weight', 'up_blocks.2.resnets.1.conv2.weight', 'up_blocks.3.resnets.2.conv2.bias', 'mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight', 'up_blocks.3.resnets.0.norm1.weight', 'up_blocks.0.resnets.2.norm2.bias', 'down_blocks.3.resnets.0.norm1.weight', 'up_blocks.1.resnets.0.time_emb_proj.bias', 'down_blocks.1.resnets.0.conv1.bias', 'down_blocks.0.downsamplers.0.conv.bias', 'up_blocks.3.resnets.2.norm2.weight', 'mid_block.resnets.1.norm1.weight', 'up_blocks.0.resnets.1.time_emb_proj.weight', 'up_blocks.2.resnets.1.time_emb_proj.weight', 'mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'down_blocks.2.downsamplers.0.conv.bias', 'down_blocks.3.resnets.0.conv1.weight', 'up_blocks.0.resnets.0.time_emb_proj.bias', 'up_blocks.3.resnets.2.norm2.bias', 'down_blocks.3.resnets.0.norm2.bias', 'mid_block.attentions.0.proj_out.weight', 'down_blocks.0.resnets.0.conv2.weight', 'mid_block.resnets.1.time_emb_proj.bias', 'up_blocks.1.resnets.0.conv_shortcut.weight', 'mid_block.resnets.1.norm2.weight', 'up_blocks.2.resnets.2.norm2.bias', 'mid_block.resnets.1.conv1.bias', 'down_blocks.2.downsamplers.0.conv.weight', 'down_blocks.2.resnets.1.conv1.bias', 'mid_block.attentions.0.proj_in.bias', 'time_embedding.linear_1.bias', 'down_blocks.0.resnets.1.norm2.weight', 'up_blocks.0.resnets.1.conv2.weight', 'down_blocks.1.resnets.0.conv1.weight', 'up_blocks.1.resnets.0.norm1.weight', 'up_blocks.2.resnets.0.conv1.weight', 'mid_block.attentions.0.norm.bias', 'up_blocks.3.resnets.1.norm1.bias', 'mid_block.resnets.0.conv2.weight', 'mid_block.attentions.0.transformer_blocks.0.norm1.weight', 'conv_norm_out.bias', 'down_blocks.3.resnets.1.conv1.weight', 'down_blocks.2.resnets.0.conv2.bias', 'up_blocks.0.resnets.2.conv1.weight', 'mid_block.resnets.1.conv1.weight', 'mid_block.attentions.0.proj_in.weight', 'down_blocks.1.resnets.0.norm2.weight', 'mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'up_blocks.1.resnets.2.conv1.weight', 'up_blocks.1.resnets.2.conv1.bias', 'down_blocks.3.resnets.1.norm1.weight', 'up_blocks.1.resnets.2.norm1.bias', 'up_blocks.3.resnets.1.norm1.weight', 'up_blocks.2.upsamplers.0.conv.bias', 'up_blocks.1.resnets.0.conv1.weight', 'down_blocks.0.resnets.0.norm1.bias', 'up_blocks.0.resnets.2.norm1.bias', 'up_blocks.2.resnets.1.norm2.weight', 'up_blocks.2.resnets.0.norm2.bias', 'mid_block.resnets.1.time_emb_proj.weight', 'mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'down_blocks.2.resnets.0.conv1.weight', 'up_blocks.3.resnets.2.time_emb_proj.weight', 'down_blocks.0.resnets.1.norm1.weight', 'up_blocks.0.resnets.2.conv2.bias', 'up_blocks.3.resnets.0.norm1.bias', 'down_blocks.0.resnets.0.conv2.bias', 'mid_block.resnets.1.conv2.weight', 'up_blocks.3.resnets.1.conv1.weight', 'up_blocks.3.resnets.1.norm2.bias', 'down_blocks.0.resnets.1.norm2.bias', 'down_blocks.3.resnets.1.norm2.weight', 'up_blocks.2.resnets.0.norm2.weight', 'down_blocks.2.resnets.0.norm1.weight', 'up_blocks.1.resnets.1.conv1.weight', 'up_blocks.2.resnets.2.conv1.weight', 'mid_block.attentions.0.transformer_blocks.0.norm3.weight', 'up_blocks.1.resnets.0.conv2.bias', 'up_blocks.3.resnets.0.time_emb_proj.bias', 'mid_block.resnets.1.norm1.bias', 'down_blocks.1.resnets.1.conv2.weight', 'mid_block.resnets.1.conv2.bias', 'up_blocks.0.resnets.0.conv1.bias', 'up_blocks.0.resnets.2.conv2.weight', 'up_blocks.2.resnets.1.norm1.bias', 'up_blocks.0.resnets.1.conv2.bias', 'mid_block.resnets.0.norm2.bias', 'up_blocks.1.resnets.2.norm2.weight', 'time_embedding.linear_2.weight', 'conv_out.weight', 'up_blocks.1.resnets.2.conv2.weight', 'mid_block.resnets.0.norm2.weight', 'up_blocks.2.resnets.0.conv2.weight', 'up_blocks.1.resnets.1.conv2.bias', 'up_blocks.2.resnets.2.conv2.weight', 'down_blocks.1.resnets.1.norm2.weight', 'up_blocks.1.resnets.2.time_emb_proj.bias', 'up_blocks.2.resnets.0.norm1.weight', 'down_blocks.2.resnets.1.norm1.weight', 'up_blocks.0.upsamplers.0.conv.weight', 'up_blocks.2.resnets.2.norm1.bias', 'mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'up_blocks.2.resnets.0.time_emb_proj.bias', 'up_blocks.1.resnets.0.conv1.bias', 'up_blocks.1.resnets.0.conv2.weight', 'down_blocks.1.resnets.1.conv2.bias', 'up_blocks.1.resnets.0.norm2.bias', 'down_blocks.2.resnets.1.norm2.bias', 'down_blocks.2.resnets.1.conv2.weight', 'down_blocks.1.resnets.1.norm1.bias', 'mid_block.attentions.0.proj_out.bias', 'down_blocks.0.resnets.0.norm1.weight', 'down_blocks.0.resnets.1.conv1.weight', 'up_blocks.1.resnets.2.conv2.bias', 'up_blocks.0.resnets.2.norm2.weight', 'mid_block.attentions.0.transformer_blocks.0.norm3.bias', 'mid_block.resnets.1.norm2.bias', 'down_blocks.2.resnets.0.conv_shortcut.bias', 'up_blocks.1.upsamplers.0.conv.weight', 'down_blocks.1.resnets.0.conv2.weight', 'up_blocks.0.resnets.1.norm2.weight', 'down_blocks.2.resnets.0.conv_shortcut.weight', 'up_blocks.3.resnets.1.conv2.weight', 'down_blocks.2.resnets.0.conv1.bias', 'mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'down_blocks.0.resnets.0.norm2.weight', 'up_blocks.3.resnets.0.norm2.bias', 'up_blocks.0.resnets.0.conv2.weight', 'up_blocks.0.resnets.1.time_emb_proj.bias', 'mid_block.attentions.0.transformer_blocks.0.norm1.bias', 'conv_out.bias', 'down_blocks.1.resnets.1.norm1.weight', 'up_blocks.2.resnets.0.conv_shortcut.bias', 'up_blocks.2.resnets.0.conv1.bias', 'mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'down_blocks.1.resnets.0.conv_shortcut.weight', 'down_blocks.1.resnets.1.conv1.bias', 'up_blocks.0.resnets.1.conv1.bias', 'mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'up_blocks.0.resnets.1.norm1.bias', 'down_blocks.3.resnets.0.conv2.bias', 'mid_block.resnets.0.time_emb_proj.bias', 'time_embedding.linear_1.weight', 'down_blocks.0.resnets.1.conv2.bias', 'up_blocks.1.resnets.1.conv1.bias', 'up_blocks.1.resnets.0.conv_shortcut.bias', 'mid_block.resnets.0.conv2.bias', 'down_blocks.0.resnets.0.conv1.weight', 'down_blocks.3.resnets.0.conv2.weight', 'up_blocks.3.resnets.2.norm1.weight', 'mid_block.resnets.0.conv1.weight', 'up_blocks.1.resnets.1.time_emb_proj.bias', 'down_blocks.1.resnets.0.norm2.bias', 'up_blocks.0.resnets.2.norm1.weight', 'up_blocks.0.resnets.1.norm1.weight', 'up_blocks.3.resnets.0.conv1.bias', 'up_blocks.3.resnets.1.conv2.bias', 'down_blocks.0.resnets.1.conv2.weight', 'down_blocks.0.resnets.1.norm1.bias', 'down_blocks.2.resnets.0.conv2.weight', 'up_blocks.1.resnets.1.conv2.weight', 'down_blocks.3.resnets.0.conv1.bias', 'up_blocks.0.resnets.1.norm2.bias', 'down_blocks.1.resnets.0.conv_shortcut.bias', 'mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'conv_norm_out.weight', 'up_blocks.2.resnets.1.conv1.weight', 'up_blocks.2.resnets.1.time_emb_proj.bias', 'mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'time_embedding.linear_2.bias', 'up_blocks.3.resnets.1.time_emb_proj.weight', 'mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias', 'up_blocks.3.resnets.0.conv2.weight', 'up_blocks.2.resnets.0.conv2.bias', 'down_blocks.3.resnets.1.norm2.bias', 'down_blocks.2.resnets.1.norm2.weight', 'down_blocks.1.resnets.0.norm1.weight', 'up_blocks.2.resnets.2.conv1.bias', 'up_blocks.1.resnets.1.norm1.weight', 'up_blocks.2.upsamplers.0.conv.weight', 'up_blocks.0.resnets.0.conv2.bias', 'up_blocks.1.resnets.1.norm2.weight', 'up_blocks.0.resnets.0.time_emb_proj.weight', 'conv_in.weight', 'up_blocks.3.resnets.0.time_emb_proj.weight', 'up_blocks.0.resnets.0.norm1.weight', 'up_blocks.2.resnets.2.time_emb_proj.weight', 'mid_block.resnets.0.conv1.bias', 'up_blocks.0.resnets.2.time_emb_proj.bias', 'down_blocks.0.resnets.0.norm2.bias', 'mid_block.attentions.0.transformer_blocks.0.norm2.weight', 'up_blocks.3.resnets.1.conv1.bias', 'up_blocks.2.resnets.1.norm1.weight', 'up_blocks.3.resnets.2.norm1.bias', 'mid_block.resnets.0.norm1.weight', 'down_blocks.3.resnets.1.norm1.bias', 'up_blocks.2.resnets.0.conv_shortcut.weight', 'down_blocks.2.resnets.0.norm2.weight', 'down_blocks.3.resnets.1.conv2.weight', 'up_blocks.1.resnets.2.norm2.bias', 'up_blocks.3.resnets.0.norm2.weight', 'mid_block.resnets.0.norm1.bias', 'up_blocks.3.resnets.2.conv2.weight', 'up_blocks.0.resnets.0.conv1.weight', 'up_blocks.1.resnets.0.norm1.bias', 'up_blocks.0.resnets.2.conv1.bias', 'mid_block.attentions.0.norm.weight', 'up_blocks.0.resnets.2.time_emb_proj.weight', 'down_blocks.1.resnets.1.norm2.bias', 'mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'up_blocks.1.resnets.2.norm1.weight', 'down_blocks.0.resnets.0.conv1.bias', 'up_blocks.0.resnets.0.norm1.bias', 'up_blocks.3.resnets.2.conv1.weight', 'up_blocks.3.resnets.0.conv1.weight', 'mid_block.resnets.0.time_emb_proj.weight', 'down_blocks.3.resnets.1.conv1.bias', 'up_blocks.0.resnets.1.conv1.weight', 'up_blocks.0.resnets.0.norm2.bias', 'up_blocks.2.resnets.1.conv2.bias', 'down_blocks.2.resnets.0.norm2.bias', 'up_blocks.1.resnets.1.norm2.bias', 'down_blocks.0.resnets.1.conv1.bias', 'up_blocks.3.resnets.1.norm2.weight', 'up_blocks.0.upsamplers.0.conv.bias', 'up_blocks.2.resnets.1.norm2.bias', 'up_blocks.3.resnets.2.conv1.bias', 'conv_in.bias', 'up_blocks.3.resnets.1.time_emb_proj.bias', 'down_blocks.0.downsamplers.0.conv.weight', 'down_blocks.1.resnets.1.conv1.weight', 'down_blocks.3.resnets.1.conv2.bias', 'down_blocks.2.resnets.1.norm1.bias', 'up_blocks.1.resnets.1.time_emb_proj.weight', 'down_blocks.1.resnets.0.conv2.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tokenizer output shape: torch.Size([1, 77])\n",
      "✓ Text encoder output shape: torch.Size([1, 77, 512])\n",
      "✓ Initial latents shape: torch.Size([1, 4, 64, 64])\n",
      "\n",
      "Denoising step 980\n",
      "✓ Scaled model input shape: torch.Size([1, 4, 64, 64])\n",
      "\n",
      "Error in UNet forward pass:\n",
      "Input shapes:\n",
      "- latent_model_input: torch.Size([1, 4, 64, 64])\n",
      "- timestep: torch.Size([])\n",
      "- text_embeddings: torch.Size([1, 77, 512])\n",
      "\n",
      "❌ Error during generation: Given groups=1, weight of size [128, 3, 3, 3], expected input[1, 4, 64, 64] to have 3 channels, but got 4 channels instead\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "    \n",
    "try:\n",
    "        prompt = \"a beautiful sunset over mountains\"\n",
    "        print(f\"\\nGenerating image for prompt: '{prompt}'\")\n",
    "        generated_image = generate_image(prompt)\n",
    "        \n",
    "        from torchvision.utils import save_image\n",
    "        save_image(generated_image, \"generated_image.png\")\n",
    "        print(\"\\nImage saved successfully!\")\n",
    "        \n",
    "except Exception as e:\n",
    "        print(f\"\\n❌ Error during generation: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
