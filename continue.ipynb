{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the code in this notebook to finetune the model at a certain epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from diffusers import UNet2DConditionModel, DDPMScheduler, AutoencoderKL\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "from torchvision.transforms import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parameters\n",
    "batch_size = 8\n",
    "EPOCHS = 2  # SET IT ACCORDINGLY\n",
    "learning_rate = 1e-4\n",
    "image_size = 64\n",
    "latent_size = image_size // 8\n",
    "data_path = \"/teamspace/studios/this_studio/coco\"\n",
    "checkpoint_to_resume = \"./ldm_checkpoints/epoch_2\"  # Specify your checkpoint\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Tokenizer and Transforms\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "max_length = 77\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class (same as your training script)\n",
    "class CocoWithAnnotations(Dataset):\n",
    "    def __init__(self, path, tokenizer, transform, train=True):\n",
    "        super().__init__()\n",
    "        self.path = path\n",
    "        self.data = None\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.train = train\n",
    "        if self.data is None:\n",
    "            self.open_json()\n",
    "\n",
    "    def open_json(self):\n",
    "        split = \"train\" if self.train else \"val\"\n",
    "        with open(f'{self.path}/annotations/captions_{split}2014.json', 'r') as stream:\n",
    "            self.data = json.load(stream)['annotations']\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        annot = self.data[index]\n",
    "        img_id = str(annot['image_id']).zfill(12)\n",
    "        split = \"train\" if self.train else \"val\"\n",
    "        image_path = f'{self.path}/{split}2014/COCO_{split}2014_{img_id}.jpg'\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        text_emb = self.tokenizer(\n",
    "            annot['caption'],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return image, text_emb.input_ids.squeeze(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "# DataLoader\n",
    "def collate_fn(batch):\n",
    "    batch = [(img, cap) for img, cap in batch if img is not None and cap is not None]\n",
    "    if not batch:\n",
    "        return None\n",
    "    images, captions = zip(*batch)\n",
    "    return torch.stack(images), torch.stack(captions)\n",
    "\n",
    "dataset = CocoWithAnnotations(data_path, tokenizer, transform, train=True)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n",
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c18ee6a35da40159a579c024ef18de1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/4:   0%|          | 0/51765 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 completed with average loss: 0.1888\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04b1f8fe48904f648df675299d81f6cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/4:   0%|          | 0/51765 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 completed with average loss: 0.1875\n",
      "Training resumed and completed!\n"
     ]
    }
   ],
   "source": [
    "# Models\n",
    "unet = UNet2DConditionModel.from_pretrained(os.path.join(checkpoint_to_resume, \"unet\")).to(device)\n",
    "autoencoder = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\").to(device)\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "diffusion = DDPMScheduler(num_train_timesteps=1000, beta_schedule=\"linear\")\n",
    "\n",
    "# Freeze VAE and text encoder\n",
    "for param in autoencoder.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in text_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(unet.parameters(), lr=learning_rate)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(os.path.join(checkpoint_to_resume, \"training_state.pth\"))\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "START_EPOCH = checkpoint['epoch'] + 1  # Start from the next epoch\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(START_EPOCH, START_EPOCH + EPOCHS):\n",
    "    unet.train()\n",
    "    epoch_loss = 0\n",
    "    valid_batches = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{START_EPOCH + EPOCHS}\")\n",
    "    for batch in progress_bar:\n",
    "        if batch is None:\n",
    "            continue\n",
    "\n",
    "        images, captions = batch\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            with torch.no_grad():\n",
    "                latents = autoencoder.encode(images).latent_dist.sample()\n",
    "                latents = latents * 0.18215\n",
    "\n",
    "            noise = torch.randn_like(latents)\n",
    "            timesteps = torch.randint(0, diffusion.config.num_train_timesteps, (latents.shape[0],), device=device)\n",
    "            noisy_latents = diffusion.add_noise(latents, noise, timesteps)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                encoder_hidden_states = text_encoder(captions)[0]\n",
    "\n",
    "            noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "            loss = torch.nn.functional.mse_loss(noise_pred, noise, reduction=\"mean\")\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(unet.parameters(), 0.5)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        valid_batches += 1\n",
    "        progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    avg_loss = epoch_loss / valid_batches\n",
    "    print(f\"Epoch {epoch + 1} completed with average loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    checkpoint_dir = f\"./ldm_checkpoints/epoch_{epoch + 1}\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    unet.save_pretrained(os.path.join(checkpoint_dir, \"unet\"))\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': unet.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': avg_loss,\n",
    "    }, os.path.join(checkpoint_dir, \"training_state.pth\"))\n",
    "\n",
    "print(\"Training resumed and completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
